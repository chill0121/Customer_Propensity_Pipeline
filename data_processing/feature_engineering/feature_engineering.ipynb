{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94823f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session (example)\n",
    "def get_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"CreditUnionFeatureEngineering\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "\n",
    "class CreditUnionFeatureEngineering:\n",
    "    \"\"\"\n",
    "    PySpark-based feature engineering pipeline for credit union data\n",
    "    Designed for rolling snapshots and ML model preparation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session: SparkSession):\n",
    "        self.spark = spark_session\n",
    "        self.logger = self._setup_logging()\n",
    "        \n",
    "    def _setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    # def load_data(self, \n",
    "    #               customer_df: DataFrame,\n",
    "    #               transaction_df: DataFrame, \n",
    "    #               interaction_df: DataFrame,\n",
    "    #               churn_df: DataFrame) -> dict[str, DataFrame]:\n",
    "    #     \"\"\"Load and validate input dataframes\"\"\"\n",
    "        \n",
    "    #     # Cache frequently accessed dataframes\n",
    "    #     customer_df.cache()\n",
    "    #     transaction_df.cache()\n",
    "    #     interaction_df.cache()\n",
    "    #     churn_df.cache()\n",
    "        \n",
    "    #     self.logger.info(f\"Loaded data - Customers: {customer_df.count()}, \"\n",
    "    #                     f\"Transactions: {transaction_df.count()}, \"\n",
    "    #                     f\"Interactions: {interaction_df.count()}\")\n",
    "        \n",
    "    #     return {\n",
    "    #         'customers': customer_df,\n",
    "    #         'transactions': transaction_df,\n",
    "    #         'interactions': interaction_df,\n",
    "    #         'churn': churn_df\n",
    "    #     }\n",
    "    def load_data(self, \n",
    "                  customer_df: DataFrame = None,\n",
    "                  transaction_df: DataFrame = None, \n",
    "                  interaction_df: DataFrame = None,\n",
    "                  churn_df: DataFrame = None,\n",
    "                  data_path: str = None) -> dict[str, DataFrame]:\n",
    "        \"\"\"\n",
    "        Load and validate input dataframes\n",
    "        \n",
    "        Args:\n",
    "            customer_df, transaction_df, interaction_df, churn_df: Pre-loaded DataFrames\n",
    "            data_path: Path to parquet files (alternative to pre-loaded DataFrames)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of cached DataFrames\n",
    "        \"\"\"\n",
    "        \n",
    "        # Option 1: Load from parquet files\n",
    "        if data_path is not None:\n",
    "            self.logger.info(f\"Loading data from parquet files at: {data_path}\")\n",
    "            \n",
    "            try:\n",
    "                customer_df = self.spark.read.parquet(f\"{data_path}/customers.parquet\")\n",
    "                transaction_df = self.spark.read.parquet(f\"{data_path}/transactions.parquet\")\n",
    "                interaction_df = self.spark.read.parquet(f\"{data_path}/interactions.parquet\")\n",
    "                churn_df = self.spark.read.parquet(f\"{data_path}/churn.parquet\")\n",
    "                \n",
    "                self.logger.info(\"Successfully loaded all parquet files\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to load parquet files: {e}\")\n",
    "                raise\n",
    "        \n",
    "        # Option 2: Use pre-loaded DataFrames\n",
    "        elif all(df is not None for df in [customer_df, transaction_df, interaction_df, churn_df]):\n",
    "            self.logger.info(\"Using pre-loaded DataFrames\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Either provide all DataFrames or specify data_path for parquet files\")\n",
    "        \n",
    "        # Validate that we have data\n",
    "        if any(df is None for df in [customer_df, transaction_df, interaction_df, churn_df]):\n",
    "            raise ValueError(\"One or more DataFrames are None after loading\")\n",
    "        \n",
    "        # Cache frequently accessed dataframes\n",
    "        customer_df.cache()\n",
    "        transaction_df.cache()\n",
    "        interaction_df.cache()\n",
    "        churn_df.cache()\n",
    "        \n",
    "        # Log data counts\n",
    "        try:\n",
    "            customer_count = customer_df.count()\n",
    "            transaction_count = transaction_df.count()\n",
    "            interaction_count = interaction_df.count()\n",
    "            churn_count = churn_df.count()\n",
    "            \n",
    "            self.logger.info(f\"Loaded data - Customers: {customer_count:,}, \"\n",
    "                            f\"Transactions: {transaction_count:,}, \"\n",
    "                            f\"Interactions: {interaction_count:,}, \"\n",
    "                            f\"Churn: {churn_count:,}\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not count rows: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'customers': customer_df,\n",
    "            'transactions': transaction_df,\n",
    "            'interactions': interaction_df,\n",
    "            'churn': churn_df\n",
    "        }\n",
    "    \n",
    "    def create_transaction_features(self, \n",
    "                                  transaction_df: DataFrame,\n",
    "                                  snapshot_date: str,\n",
    "                                  lookback_days: int = 90) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create comprehensive transaction-based features for a given snapshot date\n",
    "        \"\"\"\n",
    "        \n",
    "        snapshot_dt = to_date(lit(snapshot_date))\n",
    "        cutoff_date = date_sub(snapshot_dt, lookback_days)\n",
    "        \n",
    "        # Filter transactions within lookback window\n",
    "        recent_transactions = transaction_df.filter(\n",
    "            (col(\"transaction_date\") <= snapshot_dt) & \n",
    "            (col(\"transaction_date\") >= cutoff_date)\n",
    "        )\n",
    "        \n",
    "        # Define window specifications\n",
    "        customer_window = Window.partitionBy(\"customer_id\")\n",
    "        customer_product_window = Window.partitionBy(\"customer_id\", \"product_type\")\n",
    "        \n",
    "        # Aggregate transaction features\n",
    "        transaction_features = recent_transactions.groupBy(\"customer_id\").agg(\n",
    "            # Volume metrics\n",
    "            count(\"*\").alias(\"tx_count_90d\"),\n",
    "            sum(\"amount\").alias(\"tx_total_amount_90d\"),\n",
    "            avg(\"amount\").alias(\"tx_avg_amount_90d\"),\n",
    "            stddev(\"amount\").alias(\"tx_std_amount_90d\"),\n",
    "            min(\"amount\").alias(\"tx_min_amount_90d\"),\n",
    "            max(\"amount\").alias(\"tx_max_amount_90d\"),\n",
    "            \n",
    "            # Timing metrics\n",
    "            max(\"transaction_date\").alias(\"last_tx_date\"),\n",
    "            min(\"transaction_date\").alias(\"first_tx_date\"),\n",
    "            countDistinct(\"transaction_date\").alias(\"tx_active_days_90d\"),\n",
    "            \n",
    "            # Transaction type diversity\n",
    "            countDistinct(\"transaction_type\").alias(\"tx_type_diversity_90d\"),\n",
    "            countDistinct(\"product_type\").alias(\"product_diversity_90d\"),\n",
    "            \n",
    "            # Debit/Credit patterns\n",
    "            sum(when(col(\"amount\") > 0, col(\"amount\")).otherwise(0)).alias(\"total_credits_90d\"),\n",
    "            sum(when(col(\"amount\") < 0, abs(col(\"amount\"))).otherwise(0)).alias(\"total_debits_90d\"),\n",
    "            sum(when(col(\"amount\") > 0, 1).otherwise(0)).alias(\"credit_count_90d\"),\n",
    "            sum(when(col(\"amount\") < 0, 1).otherwise(0)).alias(\"debit_count_90d\")\n",
    "        )\n",
    "        \n",
    "        # Product-specific features\n",
    "        product_features = recent_transactions.groupBy(\"customer_id\", \"product_type\").agg(\n",
    "            count(\"*\").alias(\"tx_count\"),\n",
    "            sum(\"amount\").alias(\"tx_amount\"),\n",
    "            avg(\"amount\").alias(\"tx_avg_amount\")\n",
    "        ).groupBy(\"customer_id\").pivot(\"product_type\").agg(\n",
    "            first(\"tx_count\").alias(\"count\"),\n",
    "            first(\"tx_amount\").alias(\"amount\"),\n",
    "            first(\"tx_avg_amount\").alias(\"avg_amount\")\n",
    "        )\n",
    "        \n",
    "        # Calculate derived features\n",
    "        enhanced_features = transaction_features.withColumn(\n",
    "            \"days_since_last_tx\", \n",
    "            datediff(snapshot_dt, col(\"last_tx_date\"))\n",
    "        ).withColumn(\n",
    "            \"tx_frequency_90d\",\n",
    "            col(\"tx_count_90d\") / greatest(col(\"tx_active_days_90d\"), lit(1))\n",
    "        ).withColumn(\n",
    "            \"credit_debit_ratio\",\n",
    "            when(col(\"total_debits_90d\") > 0, \n",
    "                 col(\"total_credits_90d\") / col(\"total_debits_90d\")).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"avg_days_between_tx\",\n",
    "            when(col(\"tx_count_90d\") > 1,\n",
    "                 datediff(col(\"last_tx_date\"), col(\"first_tx_date\")) / (col(\"tx_count_90d\") - 1)\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"snapshot_date\", \n",
    "            snapshot_dt\n",
    "        )\n",
    "        \n",
    "        # Join product-specific features\n",
    "        if product_features.count() > 0:\n",
    "            enhanced_features = enhanced_features.join(\n",
    "                product_features, \n",
    "                on=\"customer_id\", \n",
    "                how=\"left\"\n",
    "            )\n",
    "        \n",
    "        return enhanced_features\n",
    "    \n",
    "    def create_interaction_features(self, \n",
    "                                  interaction_df: DataFrame,\n",
    "                                  snapshot_date: str,\n",
    "                                  lookback_days: int = 90) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create customer interaction and engagement features\n",
    "        \"\"\"\n",
    "        \n",
    "        snapshot_dt = to_date(lit(snapshot_date))\n",
    "        cutoff_date = date_sub(snapshot_dt, lookback_days)\n",
    "        \n",
    "        recent_interactions = interaction_df.filter(\n",
    "            (col(\"interaction_date\") <= snapshot_dt) & \n",
    "            (col(\"interaction_date\") >= cutoff_date)\n",
    "        )\n",
    "        \n",
    "        interaction_features = recent_interactions.groupBy(\"customer_id\").agg(\n",
    "            # Login patterns\n",
    "            sum(when(col(\"interaction_type\") == \"login\", 1).otherwise(0)).alias(\"login_count_90d\"),\n",
    "            max(when(col(\"interaction_type\") == \"login\", col(\"interaction_date\"))).alias(\"last_login_date\"),\n",
    "            \n",
    "            # Support interactions\n",
    "            sum(when(col(\"interaction_type\") == \"support_call\", 1).otherwise(0)).alias(\"support_calls_90d\"),\n",
    "            sum(when(col(\"interaction_type\") == \"support_email\", 1).otherwise(0)).alias(\"support_emails_90d\"),\n",
    "            sum(when(col(\"interaction_type\") == \"support_chat\", 1).otherwise(0)).alias(\"support_chats_90d\"),\n",
    "            \n",
    "            # Digital engagement\n",
    "            sum(when(col(\"interaction_type\") == \"mobile_app\", 1).otherwise(0)).alias(\"mobile_sessions_90d\"),\n",
    "            sum(when(col(\"interaction_type\") == \"web_portal\", 1).otherwise(0)).alias(\"web_sessions_90d\"),\n",
    "            \n",
    "            # Overall engagement\n",
    "            count(\"*\").alias(\"total_interactions_90d\"),\n",
    "            countDistinct(\"interaction_type\").alias(\"interaction_diversity_90d\"),\n",
    "            countDistinct(\"interaction_date\").alias(\"active_interaction_days_90d\")\n",
    "        ).withColumn(\n",
    "            \"days_since_last_login\",\n",
    "            datediff(snapshot_dt, col(\"last_login_date\"))\n",
    "        ).withColumn(\n",
    "            \"total_support_interactions_90d\",\n",
    "            col(\"support_calls_90d\") + col(\"support_emails_90d\") + col(\"support_chats_90d\")\n",
    "        ).withColumn(\n",
    "            \"digital_engagement_score\",\n",
    "            (col(\"mobile_sessions_90d\") + col(\"web_sessions_90d\")) / greatest(col(\"total_interactions_90d\"), lit(1))\n",
    "        ).withColumn(\n",
    "            \"support_intensity_score\",\n",
    "            col(\"total_support_interactions_90d\") / greatest(col(\"total_interactions_90d\"), lit(1))\n",
    "        ).withColumn(\n",
    "            \"snapshot_date\",\n",
    "            snapshot_dt\n",
    "        )\n",
    "        \n",
    "        return interaction_features\n",
    "    \n",
    "    def create_customer_tenure_features(self, \n",
    "                                      customer_df: DataFrame,\n",
    "                                      snapshot_date: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create customer tenure and demographic-derived features\n",
    "        \"\"\"\n",
    "        \n",
    "        snapshot_dt = to_date(lit(snapshot_date))\n",
    "        \n",
    "        tenure_features = customer_df.withColumn(\n",
    "            \"account_age_days\",\n",
    "            datediff(snapshot_dt, col(\"account_open_date\"))\n",
    "        ).withColumn(\n",
    "            \"account_age_months\",\n",
    "            months_between(snapshot_dt, col(\"account_open_date\"))\n",
    "        ).withColumn(\n",
    "            \"account_age_years\",\n",
    "            col(\"account_age_months\") / 12\n",
    "        ).withColumn(\n",
    "            # Age-based features\n",
    "            \"age_group\",\n",
    "            when(col(\"age\") < 25, \"18-24\")\n",
    "            .when(col(\"age\") < 35, \"25-34\")\n",
    "            .when(col(\"age\") < 50, \"35-49\")\n",
    "            .when(col(\"age\") < 65, \"50-64\")\n",
    "            .otherwise(\"65+\")\n",
    "        ).withColumn(\n",
    "            # Income estimation (since we removed income_bracket)\n",
    "            \"estimated_income_tier\",\n",
    "            when(col(\"city\").isin([\"New York\", \"San Francisco\", \"Seattle\"]), \"high\")\n",
    "            .when(col(\"city\").isin([\"Los Angeles\", \"Chicago\", \"Boston\"]), \"medium-high\")\n",
    "            .otherwise(\"medium\")\n",
    "        ).withColumn(\n",
    "            \"snapshot_date\",\n",
    "            snapshot_dt\n",
    "        )\n",
    "        \n",
    "        return tenure_features\n",
    "    \n",
    "    def create_customer_360_snapshot(self,\n",
    "                                   data_dict: dict[str, DataFrame],\n",
    "                                   snapshot_date: str,\n",
    "                                   lookback_days: int = 90) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create comprehensive customer 360 view for a specific snapshot date\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Creating customer 360 snapshot for {snapshot_date}\")\n",
    "        \n",
    "        # Create feature sets\n",
    "        tx_features = self.create_transaction_features(\n",
    "            data_dict['transactions'], snapshot_date, lookback_days\n",
    "        )\n",
    "        \n",
    "        interaction_features = self.create_interaction_features(\n",
    "            data_dict['interactions'], snapshot_date, lookback_days\n",
    "        )\n",
    "        \n",
    "        customer_features = self.create_customer_tenure_features(\n",
    "            data_dict['customers'], snapshot_date\n",
    "        )\n",
    "        \n",
    "        # Join all features\n",
    "        customer_360 = customer_features.join(\n",
    "            tx_features, on=\"customer_id\", how=\"left\"\n",
    "        ).join(\n",
    "            interaction_features, on=\"customer_id\", how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # Fill nulls with appropriate defaults\n",
    "        numeric_columns = [field.name for field in customer_360.schema.fields \n",
    "                          if field.dataType in [IntegerType(), DoubleType(), FloatType()]]\n",
    "        \n",
    "        for col_name in numeric_columns:\n",
    "            if col_name != \"customer_id\":\n",
    "                customer_360 = customer_360.fillna({col_name: 0})\n",
    "        \n",
    "        # Add risk indicators\n",
    "        customer_360 = customer_360.withColumn(\n",
    "            \"high_risk_flag\",\n",
    "            when(\n",
    "                (col(\"days_since_last_tx\") > 30) |\n",
    "                (col(\"days_since_last_login\") > 45) |\n",
    "                (col(\"total_support_interactions_90d\") > 5),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"engagement_score\",\n",
    "            (col(\"tx_frequency_90d\") * 0.4 + \n",
    "             col(\"digital_engagement_score\") * 0.3 + \n",
    "             (1 - col(\"support_intensity_score\")) * 0.3)\n",
    "        )\n",
    "        \n",
    "        return customer_360\n",
    "    \n",
    "    def create_rolling_snapshots(self,\n",
    "                               data_dict: dict[str, DataFrame],\n",
    "                               start_date: str,\n",
    "                               end_date: str,\n",
    "                               frequency_days: int = 30,\n",
    "                               lookback_days: int = 90) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create rolling snapshots for multiple time periods\n",
    "        \"\"\"\n",
    "        \n",
    "        start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        all_snapshots = []\n",
    "        current_date = start_dt\n",
    "        \n",
    "        while current_date <= end_dt:\n",
    "            snapshot_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            self.logger.info(f\"Processing snapshot for {snapshot_date_str}\")\n",
    "            \n",
    "            snapshot = self.create_customer_360_snapshot(\n",
    "                data_dict, snapshot_date_str, lookback_days\n",
    "            )\n",
    "            \n",
    "            all_snapshots.append(snapshot)\n",
    "            current_date += timedelta(days=frequency_days)\n",
    "        \n",
    "        # Union all snapshots\n",
    "        combined_snapshots = all_snapshots[0]\n",
    "        for snapshot in all_snapshots[1:]:\n",
    "            combined_snapshots = combined_snapshots.union(snapshot)\n",
    "        \n",
    "        return combined_snapshots\n",
    "    \n",
    "    def create_feature_store_dataset(self,\n",
    "                                   customer_360_df: DataFrame,\n",
    "                                   churn_df: DataFrame,\n",
    "                                   prediction_horizon_days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create feature store dataset with target variable for ML models\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare churn labels with prediction horizon\n",
    "        churn_labels = churn_df.withColumn(\n",
    "            \"churn_prediction_date\",\n",
    "            date_sub(col(\"churn_date\"), prediction_horizon_days)\n",
    "        ).select(\n",
    "            col(\"customer_id\"),\n",
    "            col(\"churn_prediction_date\").alias(\"snapshot_date\"),\n",
    "            lit(1).alias(\"will_churn_30d\"),\n",
    "            col(\"churn_date\"),\n",
    "            col(\"churn_reason\")\n",
    "        )\n",
    "        \n",
    "        # Join features with churn labels\n",
    "        feature_store = customer_360_df.join(\n",
    "            churn_labels,\n",
    "            on=[\"customer_id\", \"snapshot_date\"],\n",
    "            how=\"left\"\n",
    "        ).fillna({\"will_churn_30d\": 0})\n",
    "        \n",
    "        # Add feature versioning\n",
    "        feature_store = feature_store.withColumn(\n",
    "            \"feature_version\",\n",
    "            lit(\"v1.0\")\n",
    "        ).withColumn(\n",
    "            \"created_timestamp\",\n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        # Ensure proper data types for ML\n",
    "        feature_store = feature_store.withColumn(\n",
    "            \"will_churn_30d\",\n",
    "            col(\"will_churn_30d\").cast(IntegerType())\n",
    "        )\n",
    "        \n",
    "        return feature_store\n",
    "\n",
    "\n",
    "# Example usage and utility functions\n",
    "def run_feature_engineering_pipeline(spark: SparkSession,\n",
    "                                    customer_df: DataFrame,\n",
    "                                    transaction_df: DataFrame,\n",
    "                                    interaction_df: DataFrame,\n",
    "                                    churn_df: DataFrame,\n",
    "                                    output_path: str = \"./data/\"):\n",
    "    \"\"\"\n",
    "    Complete feature engineering pipeline execution with parquet output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize feature engineering\n",
    "    fe = CreditUnionFeatureEngineering(spark)\n",
    "    \n",
    "    # Load data\n",
    "    data_dict = fe.load_data(customer_df, transaction_df, interaction_df, churn_df)\n",
    "    \n",
    "    # Create rolling snapshots (monthly for 2 years)\n",
    "    print(\"Creating customer 360 snapshots...\")\n",
    "    snapshots = fe.create_rolling_snapshots(\n",
    "        data_dict=data_dict,\n",
    "        start_date=\"2022-06-01\",  # Allow some history\n",
    "        end_date=\"2024-11-01\",    # Before end date to allow churn prediction\n",
    "        frequency_days=30,\n",
    "        lookback_days=90\n",
    "    )\n",
    "    \n",
    "    # Save customer 360 snapshots\n",
    "    customer_360_path = f\"{output_path}/customer_360\"\n",
    "    print(f\"Saving customer 360 snapshots to {customer_360_path}\")\n",
    "    snapshots.coalesce(4).write.mode(\"overwrite\").partitionBy(\"snapshot_date\").parquet(customer_360_path)\n",
    "    \n",
    "    # Create feature store dataset\n",
    "    print(\"Creating feature store dataset...\")\n",
    "    feature_store = fe.create_feature_store_dataset(\n",
    "        customer_360_df=snapshots,\n",
    "        churn_df=churn_df,\n",
    "        prediction_horizon_days=30\n",
    "    )\n",
    "    \n",
    "    # Save feature store\n",
    "    feature_store_path = f\"{output_path}/feature_store\"\n",
    "    print(f\"Saving feature store to {feature_store_path}\")\n",
    "    feature_store.coalesce(4).write.mode(\"overwrite\").partitionBy(\"snapshot_date\").parquet(feature_store_path)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    print(f\"Customer 360 records: {snapshots.count()}\")\n",
    "    print(f\"Feature store records: {feature_store.count()}\")\n",
    "    print(f\"Churn rate in feature store: {feature_store.filter(col('will_churn_30d') == 1).count() / feature_store.count() * 100:.2f}%\")\n",
    "    \n",
    "    return feature_store, snapshots\n",
    "\n",
    "# Utility functions for loading saved data\n",
    "def load_customer_360(spark: SparkSession, data_path: str = Path(\"...\") / \"data\" / \"processed\" / \"customer_360\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load saved customer 360 snapshots from parquet\n",
    "    \"\"\"\n",
    "    return spark.read.parquet(data_path)\n",
    "\n",
    "def load_feature_store(spark: SparkSession, data_path: str = Path(\"...\") / \"data\" / \"processed\" / \"feature_store\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load saved feature store from parquet\n",
    "    \"\"\"\n",
    "    return spark.read.parquet(data_path)\n",
    "\n",
    "def get_latest_snapshot(customer_360_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get the most recent snapshot from customer 360 data\n",
    "    \"\"\"\n",
    "    latest_date = customer_360_df.agg(max(\"snapshot_date\")).collect()[0][0]\n",
    "    return customer_360_df.filter(col(\"snapshot_date\") == latest_date)\n",
    "\n",
    "def sample_pipeline_execution():\n",
    "    \"\"\"\n",
    "    Example of how to run the complete pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Spark\n",
    "    spark = get_spark_session()\n",
    "    \n",
    "    # Assuming you have your dataframes loaded already\n",
    "    # customer_df, transaction_df, interaction_df, churn_df = load_your_simulated_data()\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(\"...\") / \"data\" / \"processed\"\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Run pipeline (uncomment when you have your data)\n",
    "    feature_store, customer_360 = run_feature_engineering_pipeline(\n",
    "        spark=spark,\n",
    "        customer_df=customer_df,\n",
    "        transaction_df=transaction_df,\n",
    "        interaction_df=interaction_df,\n",
    "        churn_df=churn_df,\n",
    "        output_path=output_path,\n",
    "        data_path=Path(\"...\") / \"data\" / \"raw\"\n",
    "    )\n",
    "    \n",
    "    # Load saved data for analysis\n",
    "    customer_360_loaded = load_customer_360(spark)\n",
    "    feature_store_loaded = load_feature_store(spark)\n",
    "    \n",
    "    # Get latest snapshot for current analysis\n",
    "    latest_snapshot = get_latest_snapshot(customer_360_loaded)\n",
    "    \n",
    "    print(\"Pipeline setup complete!\")\n",
    "    return spark\n",
    "\n",
    "# Example feature importance analysis for churn model\n",
    "def analyze_feature_importance(feature_store_df: DataFrame):\n",
    "    \"\"\"\n",
    "    Basic feature analysis for churn modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate correlation with churn target\n",
    "    numeric_features = [\n",
    "        \"tx_count_90d\", \"tx_avg_amount_90d\", \"days_since_last_tx\",\n",
    "        \"login_count_90d\", \"support_calls_90d\", \"account_age_months\",\n",
    "        \"engagement_score\", \"high_risk_flag\"\n",
    "    ]\n",
    "    \n",
    "    correlations = {}\n",
    "    for feature in numeric_features:\n",
    "        correlation = feature_store_df.stat.corr(\"will_churn_30d\", feature)\n",
    "        correlations[feature] = correlation\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2141de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_pipeline_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 450\u001b[0m, in \u001b[0;36msample_pipeline_execution\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Initialize Spark\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mget_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Assuming you have your dataframes loaded already\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# customer_df, transaction_df, interaction_df, churn_df = load_your_simulated_data()\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Create output directory\u001b[39;00m\n\u001b[1;32m    456\u001b[0m output_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mget_spark_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_spark_session\u001b[39m():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCreditUnionFeatureEngineering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.coalescePartitions.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "sample_pipeline_execution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
